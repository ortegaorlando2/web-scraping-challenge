{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "superb-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a class to be called from another file\n",
    "class myClass():\n",
    "    #create the constructor defining the variable hemispheres\n",
    "    def __init__(self,hemisph):\n",
    "        self.hemisph=hemisph\n",
    "        \n",
    "    #create the function that runs the Mars scraping script\n",
    "    def scrape(self,hemispheres):\n",
    "    # Dependencies\n",
    "\n",
    "        from bs4 import BeautifulSoup as bs\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from selenium.webdriver import Chrome\n",
    "        from selenium.webdriver.support.ui import Select\n",
    "        from selenium import webdriver\n",
    "\n",
    "        #telling the Chrome driver where is the executable\n",
    "        driver = Chrome(executable_path='C:/Webdriver/bin/chromedriver')\n",
    "\n",
    "\n",
    "        #defining a string variable with the website url\n",
    "        url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "        #Making a connection to the Nasa News website\n",
    "        r = requests.get(url)\n",
    "\n",
    "        #creating a soup object with the html data\n",
    "        soup = bs(r.text, 'html')\n",
    "\n",
    "        #Finding all the div tags in the soup\n",
    "        texto=soup.find_all('div')\n",
    "\n",
    "        # Opening the website\n",
    "        driver.get(url)\n",
    "\n",
    "\n",
    "        # getting the button by class name\n",
    "        button = driver.find_element_by_class_name(\"menu_icon\")\n",
    "\n",
    "\n",
    "        # clicking on the button\n",
    "        button.click()\n",
    "\n",
    "        #Defining a button for a reactive element which matches the request\n",
    "        button = driver.find_element_by_id(\"li_4\")\n",
    "\n",
    "\n",
    "        #click the button\n",
    "        button.click()\n",
    "\n",
    "        #saving the current url to a variable for automation\n",
    "        url2=driver.current_url\n",
    "        url2\n",
    "\n",
    "        #Connecting with the News and Events website\n",
    "        r2=requests.get(url2)\n",
    "\n",
    "        #creating a soup object with the html text from the Mars Nasa website news and events\n",
    "        soup2 = bs(r2.text, 'lxml')\n",
    "\n",
    "        #Looking for all the news and then selecting only the latest news \n",
    "        texto2=soup2.find_all('h3', class_=\"title\")\n",
    "        textoTitle=texto2[0].text.strip()\n",
    "\n",
    "        #defining the button content (reactive link) for the first news\n",
    "        button=driver.find_element_by_class_name('list_image')\n",
    "\n",
    "        #clicking button \n",
    "        button.click()\n",
    "\n",
    "        #Checking the url of the redirected website. Saving to a variable\n",
    "        url3=driver.current_url\n",
    "        url3\n",
    "\n",
    "        #connect to the website that contains the main news\n",
    "        r3=requests.get(url3)\n",
    "\n",
    "        #create a soup object for this website\n",
    "        soup3=bs(r3.text,'lxml')\n",
    "\n",
    "        #This is the first paragraph of the main text of these news\n",
    "        texto3=soup3.find('p')\n",
    "        texto3\n",
    "\n",
    "        #define a variable with the string of the Space facts website\n",
    "        url4='https://space-facts.com/mars/'\n",
    "\n",
    "        #Using requests to connect to Space Facts website\n",
    "        r4=requests.get(url4)\n",
    "\n",
    "        #creating a soup element for this website (Mars Facts)\n",
    "        soup4=bs(r4.text,'html')\n",
    "\n",
    "        #extracting html from website\n",
    "        texto4=soup4.find('table')\n",
    "        #texto4\n",
    "\n",
    "        #reading the html table from the website Mars Facts\n",
    "        table=pd.read_html(url4)\n",
    "        table[0]\n",
    "\n",
    "        #cresting a dataframe with the information extracted from Mars Facts\n",
    "        df=pd.DataFrame(table[0])\n",
    "        df.rename(columns = {0:'Property', 1:'Value'}, inplace = True)\n",
    "        df\n",
    "\n",
    "        #Converting the Mars facts to html code\n",
    "        t_html=pd.DataFrame.to_html(df)\n",
    "        #t_html\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        #website for the hemisphere images of Mars\n",
    "        url5='https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "        #executing the driver to automatically go to the website\n",
    "        driver.get(url5)\n",
    "\n",
    "        url5=driver.current_url\n",
    "        r5=requests.get(url5, timeout=10)\n",
    "        print(r5)\n",
    "\n",
    "        #Create a beautiful soup element with the the html \n",
    "        main=bs(r5.text,'lxml')\n",
    "        titles=main.find_all('div', class_='description')\n",
    "        #titles\n",
    "\n",
    "        #collecting the titles of the various images\n",
    "        titlist=[]\n",
    "\n",
    "        try:\n",
    "            for tit in titles:\n",
    "                tititem=tit.find('h3').text\n",
    "                titlist.append(tititem)\n",
    "\n",
    "        except:\n",
    "            print(\"Something went wrong\")\n",
    "            print(titlist)\n",
    "\n",
    "        #including a wait for these websites because the response is not fast\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        #getting the references from the html script\n",
    "        elems = driver.find_elements_by_css_selector(\".description [href]\")\n",
    "        links = [elem.get_attribute('href') for elem in elems]\n",
    "        links\n",
    "\n",
    "        def but1(tag,links,driver):\n",
    "            urln=links[tag]\n",
    "            driver.get(urln)\n",
    "            button=driver.find_element_by_link_text(\"Sample\")\n",
    "            button.click()\n",
    "            urlimg=driver.current_url\n",
    "            return urlimg\n",
    "        \n",
    "        \n",
    "        #Extracting the links to the images from the website (these are not the downloadable ones)\n",
    "        urlimg=[]\n",
    "        for u in range(4):\n",
    "            addurl=but1(u,links,driver)\n",
    "            urlimg.append(addurl)\n",
    "\n",
    "        #print(urlimg)\n",
    "\n",
    "        #assigning a variable to the Mars images website\n",
    "        url6='https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'\n",
    "\n",
    "        #testing the response of the website\n",
    "        r6=requests.get(url6, timeout=10)\n",
    "        r6\n",
    "\n",
    "        #inspecting the dataframe for images\n",
    "        pd.DataFrame({titlist[i]: links[i] for i in range(len(links))}, index=[0])\n",
    "\n",
    "        hemispheres=[]\n",
    "        \n",
    "        # saving a list of dictionaries with Mars images information\n",
    "        for i in range(4):\n",
    "            dic={titlist[i]:links[i]}\n",
    "            hemispheres.append(dic)\n",
    "\n",
    "        #print(hemispheres)\n",
    "        return hemispheres\n",
    "\n",
    "        #defining a function to call several times the website and click on each image\n",
    "            \n",
    "        #print(hemispheres)\n",
    "        return hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "distributed-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={\"\":\"\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "potential-basic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.myClass at 0x2ced85dca30>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marte=myClass(dict)\n",
    "Marte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-yemen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "blank-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Cerberus Hemisphere Enhanced': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced'},\n",
       " {'Schiaparelli Hemisphere Enhanced': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced'},\n",
       " {'Syrtis Major Hemisphere Enhanced': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced'},\n",
       " {'Valles Marineris Hemisphere Enhanced': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marte.scrape(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-natural",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
